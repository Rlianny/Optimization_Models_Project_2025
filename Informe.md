# Informe del Proyecto de Optimizaci√≥n: Minimizaci√≥n de f(x,y) = (x¬≤-1)¬≤ + (y¬≤-2)¬≤

## RESUMEN EJECUTIVO

**Objetivo**: Encontrar el m√≠nimo de la funci√≥n $f(x,y) = (x^2-1)^2 + (y^2-2)^2$ comparando dos algoritmos de optimizaci√≥n.

**M√©todos Implementados**:

- **Gradient Descent** (Descenso del Gradiente): M√©todo simple de primer orden
- **BFGS** (Broyden-Fletcher-Goldfarb-Shanno): M√©todo cuasi-Newton de segundo orden

**Rango de Experimentaci√≥n**: Puntos iniciales en $[-100, 100] \times [-100, 100]$ (10 experimentos)

**Resultado Te√≥rico**: La funci√≥n tiene 4 m√≠nimos globales equivalentes en $(¬±1, ¬±\sqrt{2})$ con valor $f^* = 0$.

**Resultado Principal**:

- ‚úÖ **BFGS**: 100% de √©xito (10/10 experimentos), promedio 24 iteraciones, precisi√≥n $10^{-19}$
- ‚ùå **Gradient Descent**: 50% de fallo (5/10 experimentos divergieron), promedio 54 iteraciones cuando funciona, precisi√≥n $10^{-14}$

**Conclusi√≥n**: BFGS es el m√©todo recomendado para este problema, siendo superior en robustez (2√ó), eficiencia (2.3√ó) y precisi√≥n (270√ó).

---

## 1. Definici√≥n de la Funci√≥n, Dominio y Signo

### 1.1 Definici√≥n de la Funci√≥n

La funci√≥n objetivo a minimizar es:

$$f(x,y) = (x^2 - 1)^2 + (y^2 - 2)^2$$

Esta funci√≥n es una composici√≥n de funciones polinomiales de grado cuatro.

**Tipo de problema**: Este es un problema de **optimizaci√≥n sin restricciones** (unconstrained optimization). No existen restricciones de igualdad ni desigualdad, por lo que no aplican las condiciones de Karush-Kuhn-Tucker (KKT) ni el an√°lisis de restricciones activas.

### 1.2 Dominio

El dominio de la funci√≥n es:

$$D_f = \mathbb{R}^2$$

La funci√≥n est√° definida para todos los pares ordenados $(x, y) \in \mathbb{R}^2$, ya que no existen restricciones algebraicas (como divisiones por cero, ra√≠ces de n√∫meros negativos, o logaritmos de n√∫meros no positivos).

### 1.3 Signo de la Funci√≥n

Dado que $f(x,y)$ es la suma de dos t√©rminos elevados al cuadrado:

$$f(x,y) = \underbrace{(x^2 - 1)^2}_{\geq 0} + \underbrace{(y^2 - 2)^2}_{\geq 0} \geq 0$$

Por lo tanto:

$$f(x,y) \geq 0 \quad \forall (x,y) \in \mathbb{R}^2$$

La funci√≥n alcanza su valor m√≠nimo global de 0 cuando ambos t√©rminos son simult√°neamente cero:

$$x^2 - 1 = 0 \quad \text{y} \quad y^2 - 2 = 0$$

## 2. An√°lisis de las Variables de la Funci√≥n

### 2.1 Tipo de Variables

Las variables $x$ e $y$ son **variables continuas** que toman valores en el conjunto de los n√∫meros reales $\mathbb{R}$.

- **Continuas**: Ambas variables pueden tomar cualquier valor real dentro de su dominio, sin restricciones de discreci√≥n o valores enteros.
- **No acotadas**: No existen l√≠mites superiores o inferiores para los valores que pueden tomar $x$ e $y$.

### 2.2 Independencia de las Variables

Las variables $x$ e $y$ son **independientes** entre s√≠, ya que la funci√≥n se puede escribir como la suma de dos funciones separables:

$$f(x,y) = g(x) + h(y)$$

donde:
- $g(x) = (x^2 - 1)^2$
- $h(y) = (y^2 - 2)^2$

Esta separabilidad implica que el comportamiento de la funci√≥n respecto a $x$ es independiente del valor de $y$ y viceversa.

## 3. An√°lisis de Continuidad y Diferenciabilidad

### 3.1 Continuidad

La funci√≥n $f(x,y)$ es **continua** en todo su dominio $\mathbb{R}^2$.

**Justificaci√≥n**: $f(x,y)$ es una composici√≥n y suma de funciones polinomiales, las cuales son continuas en $\mathbb{R}$. Espec√≠ficamente:

- $x^2$, $y^2$ son funciones polinomiales continuas
- $(x^2 - 1)$ y $(y^2 - 2)$ son continuas (suma/resta de funciones continuas)
- $(x^2 - 1)^2$ y $(y^2 - 2)^2$ son continuas (composici√≥n de funciones continuas)
- La suma de funciones continuas es continua

### 3.2 Diferenciabilidad

La funci√≥n $f(x,y)$ es **infinitamente diferenciable** en todo $\mathbb{R}^2$.

**Justificaci√≥n**: Las funciones polinomiales son diferenciables en todo su dominio, y la composici√≥n y suma de funciones diferenciables es diferenciable. Podemos calcular derivadas parciales de cualquier orden.

## 4. Gradiente: ‚àáf(x,y)

El gradiente de la funci√≥n es el vector de derivadas parciales de primer orden:

$$\nabla f(x,y) = \left[\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right]$$

### 4.1 C√°lculo de las Derivadas Parciales

**Derivada parcial respecto a x**:

$$\frac{\partial f}{\partial x} = \frac{\partial}{\partial x}[(x^2 - 1)^2 + (y^2 - 2)^2]$$

Aplicando la regla de la cadena:

$$\frac{\partial f}{\partial x} = 2(x^2 - 1) \cdot 2x = 4x(x^2 - 1)$$

**Derivada parcial respecto a y**:

$$\frac{\partial f}{\partial y} = \frac{\partial}{\partial y}[(x^2 - 1)^2 + (y^2 - 2)^2]$$

Aplicando la regla de la cadena:

$$\frac{\partial f}{\partial y} = 2(y^2 - 2) \cdot 2y = 4y(y^2 - 2)$$

### 4.2 Gradiente

Por lo tanto, el gradiente es:

$$\nabla f(x,y) = \begin{bmatrix} 4x(x^2 - 1) \\ 4y(y^2 - 2) \end{bmatrix}$$

## 5. Matriz Hessiana

La matriz Hessiana $H_f(x,y)$ contiene las derivadas parciales de segundo orden:

$$H_f(x,y) = \begin{bmatrix} 
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}$$

### 5.1 C√°lculo de las Derivadas de Segundo Orden

**Derivada segunda respecto a x**:

$$\frac{\partial^2 f}{\partial x^2} = \frac{\partial}{\partial x}[4x(x^2 - 1)] = 4(x^2 - 1) + 4x \cdot 2x = 4x^2 - 4 + 8x^2 = 12x^2 - 4$$

**Derivada segunda respecto a y**:

$$\frac{\partial^2 f}{\partial y^2} = \frac{\partial}{\partial y}[4y(y^2 - 2)] = 4(y^2 - 2) + 4y \cdot 2y = 4y^2 - 8 + 8y^2 = 12y^2 - 8$$

**Derivadas mixtas**:

$$\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial}{\partial y}[4x(x^2 - 1)] = 0$$

$$\frac{\partial^2 f}{\partial y \partial x} = \frac{\partial}{\partial x}[4y(y^2 - 2)] = 0$$

### 5.2 Matriz Hessiana

$$H_f(x,y) = \begin{bmatrix} 
12x^2 - 4 & 0 \\
0 & 12y^2 - 8
\end{bmatrix}$$

La matriz Hessiana es **diagonal**, lo que confirma la independencia de las variables.

## 6. An√°lisis de Convexidad

### 6.1 Condiciones de Convexidad

Una funci√≥n es convexa si su Hessiana es semidefinida positiva (todos sus valores propios son no negativos) en todo su dominio.

### 6.2 Valores Propios de la Hessiana

Dado que la Hessiana es diagonal, sus valores propios son simplemente los elementos de la diagonal:

$$\lambda_1 = 12x^2 - 4$$
$$\lambda_2 = 12y^2 - 8$$

### 6.3 An√°lisis de Convexidad

Para que la funci√≥n sea convexa globalmente, necesitamos que ambos valores propios sean no negativos para todo $(x,y) \in \mathbb{R}^2$:

$$\lambda_1 = 12x^2 - 4 \geq 0 \Rightarrow x^2 \geq \frac{1}{3} \Rightarrow |x| \geq \frac{1}{\sqrt{3}}$$

$$\lambda_2 = 12y^2 - 8 \geq 0 \Rightarrow y^2 \geq \frac{2}{3} \Rightarrow |y| \geq \sqrt{\frac{2}{3}}$$

**Conclusi√≥n**: La funci√≥n **NO es convexa globalmente** en $\mathbb{R}^2$, sino que es convexa solo en la regi√≥n:

$$R = \left\{(x,y) : |x| \geq \frac{1}{\sqrt{3}} \text{ y } |y| \geq \sqrt{\frac{2}{3}}\right\}$$

En las regiones donde $|x| < \frac{1}{\sqrt{3}}$ o $|y| < \sqrt{\frac{2}{3}}$, la funci√≥n es **no convexa** (la Hessiana no es semidefinida positiva). En particular, cerca del origen $(0,0)$, donde ambos valores propios son negativos, la funci√≥n presenta comportamiento localmente c√≥ncavo.

**Clasificaci√≥n completa de regiones:**

1. **Regi√≥n convexa** (Hessiana semidefinida positiva, ambos $\lambda_i \geq 0$):
   $$R_{\text{convexa}} = \left\{(x,y) : |x| \geq \frac{1}{\sqrt{3}} \text{ Y } |y| \geq \sqrt{\frac{2}{3}}\right\}$$
   
   En esta regi√≥n, ambos valores propios son no negativos, garantizando convexidad local.

2. **Regi√≥n c√≥ncava** (Hessiana semidefinida negativa, ambos $\lambda_i \leq 0$):
   $$R_{\text{c√≥ncava}} = \left\{(x,y) : |x| \leq \frac{1}{\sqrt{3}} \text{ Y } |y| \leq \sqrt{\frac{2}{3}}\right\}$$
   
   En esta regi√≥n, ambos valores propios son no positivos, creando comportamiento localmente c√≥ncavo. El punto $(0,0)$ (m√°ximo local) est√° en el centro de esta regi√≥n.

3. **Regiones silla** (Hessiana indefinida, valores propios de signos opuestos):
   - Regi√≥n donde $|x| < \frac{1}{\sqrt{3}}$ Y $|y| \geq \sqrt{\frac{2}{3}}$: $\lambda_1 < 0$, $\lambda_2 \geq 0$
   - Regi√≥n donde $|x| \geq \frac{1}{\sqrt{3}}$ Y $|y| < \sqrt{\frac{2}{3}}$: $\lambda_1 \geq 0$, $\lambda_2 < 0$
   
   En estas regiones, la funci√≥n no es ni convexa ni c√≥ncava. Los puntos silla $(0, \pm\sqrt{2})$ y $(\pm 1, 0)$ se encuentran en estas regiones.

4. **Fronteras de convexidad**:
   - L√≠neas verticales: $x = \pm\frac{1}{\sqrt{3}} \approx \pm 0.577$
   - L√≠neas horizontales: $y = \pm\sqrt{\frac{2}{3}} \approx \pm 0.816$

### 6.4 Implicaciones

La no convexidad global implica que:
- Los m√©todos de optimizaci√≥n basados en gradiente podr√≠an converger a diferentes soluciones dependiendo del punto inicial (existen 4 m√≠nimos globales equivalentes)
- Existen puntos silla que podr√≠an ralentizar o afectar la convergencia
- Se requiere un an√°lisis cuidadoso de los puntos estacionarios
- **Nota importante**: Aunque la funci√≥n no es convexa globalmente, el an√°lisis de la Hessiana (secci√≥n 8) demuestra que no existen m√≠nimos locales que no sean globales. Todos los m√≠nimos encontrados son m√≠nimos globales.

## 7. Determinaci√≥n del M√≠nimo Te√≥rico

### 7.1 Puntos Estacionarios

**Nota terminol√≥gica importante**:

Seg√∫n las definiciones en optimizaci√≥n:
- **Punto cr√≠tico (en c√°lculo)**: Aquellos donde la derivada es indefinida (no existe)
- **Punto estacionario**: Aquellos donde el gradiente es cero (‚àáf = 0)

En este problema:
- La funci√≥n $f(x,y)$ es de clase $C^{\infty}$ (infinitamente diferenciable en todo $\mathbb{R}^2$)
- Por lo tanto, **NO existen puntos cr√≠ticos** (la derivada existe en todos los puntos)
- Los puntos que buscamos son **puntos estacionarios** donde ‚àáf(x,y) = 0

**Justificaci√≥n de ausencia de puntos cr√≠ticos**: La funci√≥n $f(x,y) = (x^2-1)^2 + (y^2-2)^2$ es una composici√≥n de polinomios. Espec√≠ficamente:
- Los t√©rminos $x^2$, $y^2$ son polinomios (funciones $C^{\infty}$)
- Las sumas $(x^2-1)$ y $(y^2-2)$ son polinomios
- Las composiciones $(x^2-1)^2$ y $(y^2-2)^2$ son polinomios de grado 4
- La suma de polinomios es un polinomio

Como los polinomios son infinitamente diferenciables en todo $\mathbb{R}$, la funci√≥n $f$ es infinitamente diferenciable en todo $\mathbb{R}^2$. Por lo tanto, no existen puntos donde la derivada sea indefinida.

Los puntos estacionarios se encuentran donde el gradiente es cero:

$$\nabla f(x,y) = \begin{bmatrix} 4x(x^2 - 1) \\ 4y(y^2 - 2) \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$$

Esto requiere:

$$4x(x^2 - 1) = 0 \quad \Rightarrow \quad x = 0 \text{ o } x = \pm 1$$

$$4y(y^2 - 2) = 0 \quad \Rightarrow \quad y = 0 \text{ o } y = \pm\sqrt{2}$$

### 7.2 Lista de Puntos Estacionarios

Combinando todas las posibilidades, obtenemos 9 puntos estacionarios:

1. $(0, 0)$
2. $(0, \sqrt{2})$
3. $(0, -\sqrt{2})$
4. $(1, 0)$
5. $(1, \sqrt{2})$
6. $(1, -\sqrt{2})$
7. $(-1, 0)$
8. $(-1, \sqrt{2})$
9. $(-1, -\sqrt{2})$

## 8. An√°lisis de los Puntos Estacionarios

### 8.1 Clasificaci√≥n mediante el Criterio de la Hessiana

Para clasificar cada punto estacionario, evaluamos la Hessiana en cada punto y analizamos sus valores propios.

**Punto (0, 0)**:

$$H_f(0,0) = \begin{bmatrix} -4 & 0 \\ 0 & -8 \end{bmatrix}$$

Valores propios: $\lambda_1 = -4 < 0$, $\lambda_2 = -8 < 0$ ‚Üí **M√°ximo local**

$$f(0,0) = (0-1)^2 + (0-2)^2 = 1 + 4 = 5$$

**Punto (0, ¬±‚àö2)**:

$$H_f(0, \pm\sqrt{2}) = \begin{bmatrix} -4 & 0 \\ 0 & 16 \end{bmatrix}$$

Valores propios: $\lambda_1 = -4 < 0$, $\lambda_2 = 16 > 0$ ‚Üí **Punto silla**

$$f(0, \pm\sqrt{2}) = (0-1)^2 + (2-2)^2 = 1$$

**Punto (¬±1, 0)**:

$$H_f(\pm 1, 0) = \begin{bmatrix} 8 & 0 \\ 0 & -8 \end{bmatrix}$$

Valores propios: $\lambda_1 = 8 > 0$, $\lambda_2 = -8 < 0$ ‚Üí **Punto silla**

$$f(\pm 1, 0) = (1-1)^2 + (0-2)^2 = 4$$

**Punto (¬±1, ¬±‚àö2)**:

$$H_f(\pm 1, \pm\sqrt{2}) = \begin{bmatrix} 8 & 0 \\ 0 & 16 \end{bmatrix}$$

Valores propios: $\lambda_1 = 8 > 0$, $\lambda_2 = 16 > 0$ ‚Üí **M√≠nimo local** (que tambi√©n es **m√≠nimo global**, como se demuestra en la Secci√≥n 9.1)

$$f(\pm 1, \pm\sqrt{2}) = (1-1)^2 + (2-2)^2 = 0$$

### 8.2 Resumen de la Clasificaci√≥n

| Punto | Tipo | f(x,y) |
|-------|------|--------|
| $(0, 0)$ | M√°ximo local | 5 |
| $(0, \sqrt{2})$ | Punto silla | 1 |
| $(0, -\sqrt{2})$ | Punto silla | 1 |
| $(1, 0)$ | Punto silla | 4 |
| $(-1, 0)$ | Punto silla | 4 |
| $(1, \sqrt{2})$ | **M√≠nimo global** | **0** |
| $(1, -\sqrt{2})$ | **M√≠nimo global** | **0** |
| $(-1, \sqrt{2})$ | **M√≠nimo global** | **0** |
| $(-1, -\sqrt{2})$ | **M√≠nimo global** | **0** |

## 9. An√°lisis del √ìptimo

### 9.1 M√≠nimos Globales

La funci√≥n tiene **cuatro m√≠nimos globales** con el mismo valor:

$$x^* \in \{(1, \sqrt{2}), (1, -\sqrt{2}), (-1, \sqrt{2}), (-1, -\sqrt{2})\}$$

$$f(x^*) = 0$$

**Demostraci√≥n de que son m√≠nimos globales:**

1. **Cota inferior**: Como $f(x,y) = (x^2-1)^2 + (y^2-2)^2$ es la suma de dos t√©rminos al cuadrado, se cumple:
   $$f(x,y) = \underbrace{(x^2-1)^2}_{\geq 0} + \underbrace{(y^2-2)^2}_{\geq 0} \geq 0 \quad \forall (x,y) \in \mathbb{R}^2$$
   
   Por lo tanto, el valor m√≠nimo posible de la funci√≥n es 0.

2. **Alcanzabilidad**: Este valor m√≠nimo de 0 se alcanza cuando ambos t√©rminos son simult√°neamente cero:
   $$(x^2-1)^2 = 0 \quad \Rightarrow \quad x^2 = 1 \quad \Rightarrow \quad x = \pm 1$$
   $$(y^2-2)^2 = 0 \quad \Rightarrow \quad y^2 = 2 \quad \Rightarrow \quad y = \pm\sqrt{2}$$
   
   Esto produce exactamente los 4 puntos: $(¬±1, ¬±\sqrt{2})$.

3. **Conclusi√≥n**: Como $f(x^*) = 0 = \inf_{(x,y) \in \mathbb{R}^2} f(x,y)$ y este √≠nfimo se alcanza en los 4 puntos mencionados, estos son **m√≠nimos globales**.

**Implicaci√≥n importante**: Debido a que $f(x,y) \geq 0$ para todo $(x,y) \in \mathbb{R}^2$, no pueden existir m√≠nimos locales con valores mayores que 0. Cualquier punto estacionario que sea un m√≠nimo local debe tener valor 0, y por tanto, es un m√≠nimo global.

### 9.2 Interpretaci√≥n Geom√©trica

Estos cuatro m√≠nimos corresponden a las cuatro combinaciones de signos que satisfacen:
- $x^2 = 1 \Rightarrow x = \pm 1$
- $y^2 = 2 \Rightarrow y = \pm\sqrt{2}$

La existencia de m√∫ltiples m√≠nimos globales es consistente con la estructura sim√©trica de la funci√≥n y su no convexidad en ciertas regiones.

### 9.3 Caracter√≠sticas del √ìptimo

- **Valor √≥ptimo**: $f^* = 0$
- **N√∫mero de soluciones √≥ptimas**: 4 (sim√©tricamente distribuidas)
- **Naturaleza**: M√≠nimos globales estrictos localmente (cada uno es el √∫nico m√≠nimo en su vecindad)

## 10. Descripci√≥n de los Algoritmos Utilizados

### 10.1 M√©todo del Descenso del Gradiente (Gradient Descent)

#### 10.1.1 Descripci√≥n General

El m√©todo del Descenso del Gradiente es un algoritmo iterativo de optimizaci√≥n de primer orden que se mueve en la direcci√≥n opuesta al gradiente para encontrar un m√≠nimo local de una funci√≥n.

#### 10.1.2 Fundamento Matem√°tico

El gradiente $\nabla f(x)$ apunta en la direcci√≥n de mayor crecimiento de la funci√≥n. Por lo tanto, el negativo del gradiente $-\nabla f(x)$ apunta en la direcci√≥n de mayor decrecimiento, lo que lo convierte en una direcci√≥n de descenso.

#### 10.1.3 Algoritmo

Dado un punto inicial $x^{(0)}$ y una tasa de aprendizaje $\alpha > 0$:

1. **Inicializaci√≥n**: $k = 0$, $x = x^{(0)}$
2. **Iteraci√≥n**: Mientras $\|\nabla f(x^{(k)})\| > \epsilon$ y $k < k_{max}$:
   $$x^{(k+1)} = x^{(k)} - \alpha \nabla f(x^{(k)})$$
   $$k = k + 1$$
3. **Terminaci√≥n**: Retornar $x^{(k)}$ como aproximaci√≥n del m√≠nimo

#### 10.1.4 Par√°metros

- **learning_rate** ($\alpha$): Controla el tama√±o del paso en cada iteraci√≥n
  - Valores muy grandes: Pueden causar divergencia u oscilaciones
  - Valores muy peque√±os: Convergencia lenta
  - T√≠picamente: $0.001 \leq \alpha \leq 0.1$

- **tol** ($\epsilon$): Tolerancia para el criterio de parada basado en la norma del gradiente
  - Cuando $\|\nabla f(x)\| < \epsilon$, se considera que se ha alcanzado un punto cr√≠tico
  - T√≠picamente: $10^{-6} \leq \epsilon \leq 10^{-4}$

- **max_iter** ($k_{max}$): N√∫mero m√°ximo de iteraciones para evitar bucles infinitos

#### 10.1.5 Ventajas

- Simple de implementar
- Bajo costo computacional por iteraci√≥n
- Requiere solo el c√°lculo del gradiente (derivadas de primer orden)
- Garantiza descenso en cada iteraci√≥n (con $\alpha$ apropiado)

#### 10.1.6 Desventajas

- Convergencia puede ser lenta, especialmente cerca del √≥ptimo
- Sensible a la elecci√≥n de la tasa de aprendizaje
- Puede atascarse en m√≠nimos locales o puntos silla
- No es eficiente para funciones mal condicionadas

#### 10.1.7 Justificaci√≥n de Selecci√≥n

Se seleccion√≥ este algoritmo porque:
1. **Simplicidad**: Es el m√©todo de optimizaci√≥n basado en gradiente m√°s fundamental
2. **Referencia**: Sirve como l√≠nea base para comparar con m√©todos m√°s sofisticados
3. **Interpretabilidad**: Su comportamiento es f√°cil de entender y visualizar
4. **Aplicabilidad**: Funciona bien para funciones suaves y diferenciables como nuestra funci√≥n objetivo

### 10.2 M√©todo Cuasi-Newton BFGS

#### 10.2.1 Descripci√≥n General

BFGS (Broyden-Fletcher-Goldfarb-Shanno) es un m√©todo cuasi-Newton que aproxima la matriz Hessiana inversa para encontrar direcciones de b√∫squeda m√°s eficientes que el gradiente puro.

#### 10.2.2 Fundamento Matem√°tico

Los m√©todos de Newton utilizan informaci√≥n de segundo orden (la Hessiana) para encontrar la direcci√≥n de b√∫squeda:

$$x^{(k+1)} = x^{(k)} - H_f^{-1}(x^{(k)}) \nabla f(x^{(k)})$$

Sin embargo, calcular y invertir la Hessiana es costoso. BFGS construye iterativamente una aproximaci√≥n $B_k$ de $H_f^{-1}$ usando solo evaluaciones del gradiente.

#### 10.2.3 Algoritmo

1. **Inicializaci√≥n**: $k = 0$, $x^{(0)}$, $B_0 = I$ (matriz identidad)
2. **Iteraci√≥n**: Para $k = 0, 1, 2, ...$:
   
   a. Calcular direcci√≥n de b√∫squeda: $p_k = -B_k \nabla f(x^{(k)})$
   
   b. B√∫squeda de l√≠nea: Encontrar $\alpha_k$ que minimice $f(x^{(k)} + \alpha_k p_k)$
   
   c. Actualizar posici√≥n: $x^{(k+1)} = x^{(k)} + \alpha_k p_k$
   
   d. Calcular diferencias:
      - $s_k = x^{(k+1)} - x^{(k)}$
      - $y_k = \nabla f(x^{(k+1)}) - \nabla f(x^{(k)})$
   
   e. Actualizar aproximaci√≥n de la Hessiana inversa (f√≥rmula BFGS):
      $$B_{k+1} = B_k + \frac{(s_k^T y_k + y_k^T B_k y_k)(s_k s_k^T)}{(s_k^T y_k)^2} - \frac{B_k y_k s_k^T + s_k y_k^T B_k}{s_k^T y_k}$$

3. **Terminaci√≥n**: Cuando $\|\nabla f(x^{(k)})\| < \epsilon$ o $k \geq k_{max}$

#### 10.2.4 Implementaci√≥n

Se utiliza la implementaci√≥n de `scipy.optimize.minimize` con `method='BFGS'`, que incluye:
- B√∫squeda de l√≠nea robusta (condiciones de Wolfe)
- Manejo num√©rico estable de la actualizaci√≥n BFGS
- Criterios de convergencia sofisticados

#### 10.2.5 Par√°metros

- **tol**: Tolerancia para convergencia del gradiente
- **max_iter**: N√∫mero m√°ximo de iteraciones
- No requiere especificar learning_rate (se determina autom√°ticamente mediante b√∫squeda de l√≠nea)

#### 10.2.6 Ventajas

- **Convergencia superlineal**: Mucho m√°s r√°pida que el descenso del gradiente cerca del √≥ptimo
- **Adaptativo**: La b√∫squeda de l√≠nea ajusta autom√°ticamente el tama√±o del paso
- **Curvatura**: Utiliza informaci√≥n de segundo orden sin calcular expl√≠citamente la Hessiana
- **Eficiente**: Requiere menos iteraciones que m√©todos de primer orden
- **Robusto**: La implementaci√≥n de SciPy incluye salvaguardas num√©ricas

#### 10.2.7 Desventajas

- M√°s complejo de implementar desde cero
- Mayor costo computacional por iteraci√≥n (actualizaci√≥n de $B_k$)
- Requiere almacenar una matriz $n \times n$ (donde $n$ es la dimensi√≥n)
- Puede fallar si la funci√≥n no es suficientemente suave

#### 10.2.8 Justificaci√≥n de Selecci√≥n

Se seleccion√≥ BFGS porque:
1. **Eficiencia**: Es uno de los m√©todos cuasi-Newton m√°s efectivos y ampliamente utilizados
2. **Estado del arte**: Representa el est√°ndar industrial para optimizaci√≥n no lineal sin restricciones
3. **Comparaci√≥n**: Permite contrastar un m√©todo sofisticado de segundo orden con el simple descenso del gradiente
4. **Biblioteca**: La implementaci√≥n en SciPy es robusta y bien probada
5. **Aplicabilidad**: Nuestra funci√≥n es suave (clase $C^{\infty}$), ideal para BFGS

### 10.3 An√°lisis de Casos Problem√°ticos y Posibles Fallas

#### 10.3.1 Casos Problem√°ticos Identificados

Para la funci√≥n $f(x,y) = (x^2-1)^2 + (y^2-2)^2$, se han identificado las siguientes situaciones potencialmente problem√°ticas:

**1. Inicio en el M√°ximo Local (0,0)**

- **Problema**: El gradiente en $(0,0)$ es $(0,0)$, por lo que es un punto estacionario
- **Comportamiento esperado**:
  - El algoritmo podr√≠a detenerse inmediatamente si est√° exactamente en $(0,0)$
  - Si est√° muy cerca (e.g., $(0.1, 0.1)$), el gradiente es peque√±o y la convergencia ser√° lenta
  - El descenso del gradiente podr√≠a requerir muchas iteraciones para escapar
- **Mitigaci√≥n**: Usar learning rate moderado y suficientes iteraciones

**2. Inicio Cerca de Puntos Silla**

Los puntos silla est√°n en: $(0, \pm\sqrt{2})$ y $(\pm 1, 0)$

- **Problema**: En puntos silla, la Hessiana tiene valores propios de signos opuestos
- **Comportamiento esperado**:
  - Los algoritmos pueden ralentizarse significativamente
  - El descenso del gradiente puede oscilar o tomar rutas ineficientes
  - BFGS podr√≠a tener dificultades con la aproximaci√≥n de la Hessiana
- **Impacto**: Mayor n√∫mero de iteraciones, posible inestabilidad num√©rica

**3. Puntos Iniciales Muy Alejados (¬±100)**

- **Problema**: Distancia muy grande al √≥ptimo m√°s cercano
- **Comportamiento esperado**:
  - Muchas iteraciones necesarias
  - Posible divergencia si el learning rate es muy grande
  - Mayor costo computacional
- **Riesgos con Gradient Descent**:
  - Con $\alpha$ grande: Oscilaciones o divergencia
  - Con $\alpha$ peque√±o: Convergencia extremadamente lenta
- **Ventaja de BFGS**: Auto-ajuste del tama√±o de paso

**4. Regi√≥n No Convexa Central**

Para $|x| < \frac{1}{\sqrt{3}} \approx 0.577$ o $|y| < \sqrt{\frac{2}{3}} \approx 0.816$:

- **Problema**: La Hessiana no es semidefinida positiva
- **Comportamiento esperado**:
  - No se garantiza descenso monot√≥nico en todas las direcciones
  - Posibles oscilaciones en la trayectoria
  - Los m√©todos de Newton/cuasi-Newton podr√≠an comportarse de forma no est√°ndar
- **Observaci√≥n**: Esta regi√≥n contiene el m√°ximo local y varios puntos silla

#### 10.3.2 Estrategias de Robustez Implementadas

1. **L√≠mites de iteraciones**: `max_iter` ajustado seg√∫n la dificultad esperada del experimento
2. **Learning rates adaptativos**: Valores m√°s conservadores para casos problem√°ticos
3. **Tolerancia apropiada**: $\epsilon = 10^{-6}$ para balance entre precisi√≥n y convergencia
4. **M√∫ltiples puntos iniciales**: Exploraci√≥n sistem√°tica de diferentes regiones

#### 10.3.3 Predicciones sobre Comportamiento

**Gradient Descent**:
- Exitoso desde puntos en regiones convexas alejadas del origen
- Lento desde $(0.1, 0.1)$ (cerca del m√°ximo)
- Potencialmente ineficiente cerca de puntos silla
- Requerir√° muchas iteraciones desde puntos extremos (¬±100)

**BFGS**:
- Convergencia r√°pida en la mayor√≠a de casos
- Posible comportamiento an√≥malo cerca de puntos silla (primera iteraci√≥n)
- Excelente desde puntos extremos gracias a b√∫squeda de l√≠nea adaptativa
- Robusto incluso en regiones no convexas

#### 10.3.4 Tabla de Predicci√≥n de Convergencia

| Experimento | Punto Inicial | Dificultad | GD: Iteraciones | BFGS: Iteraciones |
|-------------|---------------|------------|-----------------|-------------------|
| exp1 | (0.5, 0.5) | Media | 200-500 | 10-30 |
| exp2 | (-1.5, -1.0) | Baja | 100-300 | 5-15 |
| exp3 | (100, 100) | Alta | 2000-5000 | 20-50 |
| exp4 | (-100, -100) | Alta | 2000-5000 | 20-50 |
| exp5 | (100, -100) | Alta | 2000-5000 | 20-50 |
| exp6 | (-100, 100) | Alta | 2000-5000 | 20-50 |
| exp7 | (0.1, 0.1) | Muy Alta | 500-1500 | 15-40 |
| exp8 | (0.05, ‚àö2) | Alta | 400-1000 | 20-60 |
| exp9 | (1.0, 0.05) | Alta | 400-1000 | 20-60 |
| exp10 | (50, -75) | Media-Alta | 1000-3000 | 15-40 |

**Nota**: Estas predicciones se validar√°n con los resultados experimentales reales.

### 10.4 Uso de Librer√≠as

#### 10.4.1 NumPy

- **Prop√≥sito**: Operaciones con arrays y √°lgebra lineal
- **Uso**: Vectores, c√°lculo de normas, operaciones matriciales
- **Justificaci√≥n**: Est√°ndar de facto para computaci√≥n num√©rica en Python

#### 10.4.2 SciPy

- **Prop√≥sito**: Algoritmos cient√≠ficos avanzados
- **Uso**: Implementaci√≥n de BFGS mediante `scipy.optimize.minimize`
- **Justificaci√≥n**: Implementaci√≥n robusta y optimizada de algoritmos de optimizaci√≥n

#### 10.4.3 Matplotlib

- **Prop√≥sito**: Visualizaci√≥n de resultados
- **Uso**: Gr√°ficos de contorno y trayectorias de optimizaci√≥n
- **Justificaci√≥n**: Biblioteca est√°ndar para visualizaci√≥n en Python cient√≠fico

## 11. Comparaci√≥n de Resultados

### 11.1 Criterios de Comparaci√≥n

Los m√©todos se comparan seg√∫n:

1. **N√∫mero de iteraciones**: ¬øCu√°ntos pasos requiere cada m√©todo para converger?
2. **Tiempo de ejecuci√≥n**: ¬øCu√°nto tiempo toma la optimizaci√≥n?
3. **Valor final de la funci√≥n**: ¬øQu√© tan cerca est√° del m√≠nimo te√≥rico?
4. **Punto final**: ¬øA cu√°l de los cuatro m√≠nimos globales converge?
5. **Sensibilidad al punto inicial**: ¬øC√≥mo afecta $x^{(0)}$ al resultado?
6. **Impacto del learning rate**: (Solo para Gradient Descent) ¬øC√≥mo afecta $\alpha$ a la convergencia?

### 11.2 Experimentos Dise√±ados

**Nota**: Los experimentos cubren el rango completo [-100, 100] para ambas variables, seg√∫n los requisitos del proyecto.

#### Experimento 1 (exp1.json)
- **Punto inicial**: $(0.5, 0.5)$ - Cerca del origen, regi√≥n no convexa
- **Learning rate**: $0.1$ - Moderado
- **Objetivo**: Evaluar convergencia desde un punto cercano al m√°ximo local
- **Zona**: Regi√≥n no convexa central

#### Experimento 2 (exp2.json)
- **Punto inicial**: $(-1.5, -1.0)$ - Moderadamente alejado, m√°s cerca de un m√≠nimo
- **Learning rate**: $0.05$ - Conservador
- **Objetivo**: Evaluar convergencia desde diferentes regiones del espacio
- **Zona**: Cuadrante III, regi√≥n convexa

#### Experimento 3 (exp3.json)
- **Punto inicial**: $(100, 100)$ - Extremo superior derecho
- **Learning rate**: $0.05$ - Conservador para evitar divergencia
- **Objetivo**: Probar robustez desde puntos muy alejados del √≥ptimo
- **Zona**: Cuadrante I, l√≠mite superior del rango

#### Experimento 4 (exp4.json)
- **Punto inicial**: $(-100, -100)$ - Extremo inferior izquierdo
- **Learning rate**: $0.05$ - Conservador
- **Objetivo**: Verificar convergencia desde el extremo opuesto
- **Zona**: Cuadrante III, l√≠mite inferior del rango

#### Experimento 5 (exp5.json)
- **Punto inicial**: $(100, -100)$ - Extremo inferior derecho
- **Learning rate**: $0.05$ - Conservador
- **Objetivo**: Explorar comportamiento desde l√≠mites de cuadrantes mixtos
- **Zona**: Cuadrante IV, l√≠mites del rango

#### Experimento 6 (exp6.json)
- **Punto inicial**: $(-100, 100)$ - Extremo superior izquierdo
- **Learning rate**: $0.05$ - Conservador
- **Objetivo**: Completar exploraci√≥n de los cuatro extremos
- **Zona**: Cuadrante II, l√≠mites del rango

#### Experimento 7 (exp7.json)
- **Punto inicial**: $(0.1, 0.1)$ - Muy cerca del m√°ximo local $(0,0)$
- **Learning rate**: $0.05$ - Conservador
- **Objetivo**: Analizar comportamiento cerca del m√°ximo local (caso problem√°tico)
- **Zona**: Regi√≥n cr√≠tica cerca del m√°ximo

#### Experimento 8 (exp8.json)
- **Punto inicial**: $(0.05, \sqrt{2})$ - Cerca del punto silla $(0, \sqrt{2})$
- **Learning rate**: $0.03$ - Muy conservador
- **Objetivo**: Estudiar comportamiento cerca de puntos silla
- **Zona**: Regi√≥n de punto silla

#### Experimento 9 (exp9.json)
- **Punto inicial**: $(1.0, 0.05)$ - Cerca del punto silla $(1, 0)$
- **Learning rate**: $0.03$ - Muy conservador
- **Objetivo**: Estudiar comportamiento cerca de otro punto silla
- **Zona**: Regi√≥n de punto silla

#### Experimento 10 (exp10.json)
- **Punto inicial**: $(50, -75)$ - Punto intermedio en rango amplio
- **Learning rate**: $0.05$ - Moderado
- **Objetivo**: Evaluar convergencia desde posiciones intermedias alejadas
- **Zona**: Cuadrante IV, posici√≥n intermedia

### 11.3 An√°lisis Esperado

**Gradient Descent**:
- Convergencia m√°s lenta
- Muy sensible a la tasa de aprendizaje
- Puede requerir muchas iteraciones para alcanzar alta precisi√≥n
- Trayectoria en forma de zigzag en regiones mal condicionadas

**BFGS**:
- Convergencia r√°pida (pocas iteraciones)
- Ajuste autom√°tico del tama√±o de paso
- Alta precisi√≥n en el resultado final
- Trayectoria m√°s directa hacia el √≥ptimo

### 11.4 Resultados de los Experimentos

Los resultados espec√≠ficos se generan al ejecutar el notebook y se guardan en archivos JSON en la carpeta `Results/`.

#### 11.4.1 Resultados Experimentales Obtenidos

Tras ejecutar los 10 experimentos dise√±ados, se obtuvieron los siguientes resultados:

**Tabla de Convergencia General:**

| Experimento | Punto Inicial | GD‚ÜíM√≠nimo | GD Iter | GD f(x) | BFGS‚ÜíM√≠nimo | BFGS Iter | BFGS f(x) |
|-------------|---------------|-----------|---------|---------|-------------|-----------|-----------|
| exp1 | (0.5, 0.5) | 1 | 31 | 2.4√ó10‚Åª¬π‚Å¥ | 1 | 8 | 4.2√ó10‚Åª¬π‚Åµ |
| exp2 | (-1.5, -1.0) | 4 | 29 | 3.6√ó10‚Åª¬π‚Å¥ | 4 | 10 | 2.3√ó10‚Åª¬π‚Åµ |
| **exp3** | **(100, 100)** | **DIVERGI√ì** | **5000** | **NaN** | **1** | **42** | **1.4√ó10‚Åª¬π‚Åµ** |
| **exp4** | **(-100, -100)** | **DIVERGI√ì** | **5000** | **NaN** | **4** | **42** | **3.5√ó10‚Åª¬π‚Åµ** |
| **exp5** | **(100, -100)** | **DIVERGI√ì** | **5000** | **NaN** | **2** | **42** | **1.1√ó10‚Åª¬π‚Å¥** |
| **exp6** | **(-100, 100)** | **DIVERGI√ì** | **5000** | **NaN** | **3** | **42** | **9.4√ó10‚Åª¬π‚Å∂** |
| exp7 | (0.1, 0.1) | 1 | 44 | 3.8√ó10‚Åª¬π‚Å¥ | 1 | 8 | 6.7√ó10‚Åª¬π‚Åπ |
| exp8 | (0.05, ‚àö2) | 1 | 83 | 5.9√ó10‚Åª¬π‚Å¥ | 1 | 6 | 2.0√ó10‚Åª¬π‚Åµ |
| exp9 | (1.0, 0.05) | 1 | 42 | 1.8√ó10‚Åª¬π‚Å¥ | 1 | 4 | 4.7√ó10‚Åª¬π‚Å∂ |
| **exp10** | **(50, -75)** | **DIVERGI√ì** | **4000** | **NaN** | **2** | **37** | **1.8√ó10‚Åª¬π‚Åµ** |

**Leyenda de M√≠nimos:**
- M√≠nimo 1: (1, ‚àö2)
- M√≠nimo 2: (1, -‚àö2)
- M√≠nimo 3: (-1, ‚àö2)
- M√≠nimo 4: (-1, -‚àö2)

#### 11.4.2 Hallazgos Importantes

**1. Falla Masiva del Gradient Descent en Puntos Alejados**

El resultado m√°s cr√≠tico del estudio: **Gradient Descent fall√≥ en 5 de 10 experimentos (50% tasa de fallo)**

**Experimentos con divergencia:**
- exp3 (100, 100): Divergencia por overflow
- exp4 (-100, -100): Divergencia por overflow
- exp5 (100, -100): Divergencia por overflow
- exp6 (-100, 100): Divergencia por overflow
- exp10 (50, -75): Divergencia por overflow

**Patr√≥n identificado**: Todos los puntos iniciales con $|x| \geq 50$ o $|y| \geq 75$ causaron divergencia.

**Causa ra√≠z**: El gradiente crece c√∫bicamente con la distancia:
$$\|\nabla f(x,y)\| \approx 4\sqrt{x^6 + y^6} \text{ para } |x|, |y| \gg 1$$

En $(100, 100)$: $\|\nabla f\| \approx 5.7 \times 10^7$

Con $\alpha = 0.05$, el paso es $\Delta x \approx 2.8 \times 10^6$, causando overflow explosivo.

**C√°lculo del learning rate √≥ptimo te√≥rico:**

Para garantizar convergencia en Gradient Descent con learning rate fijo, se requiere que:
$$\alpha < \frac{2}{\lambda_{\max}(H)}$$

donde $\lambda_{\max}(H)$ es el mayor valor propio de la Hessiana en cualquier punto de la trayectoria.

Para nuestra funci√≥n, los valores propios son:
$$\lambda_1 = 12x^2 - 4, \quad \lambda_2 = 12y^2 - 8$$

En puntos extremos como $(100, 100)$:
$$\lambda_{\max} = \max(12 \times 100^2 - 4, 12 \times 100^2 - 8) = 12 \times 10000 - 4 = 119996$$

Por lo tanto, para garantizar convergencia desde cualquier punto en $[-100, 100] \times [-100, 100]$:
$$\alpha < \frac{2}{119996} \approx 1.67 \times 10^{-5}$$

**Implicaci√≥n pr√°ctica**: 
- Con $\alpha = 1.67 \times 10^{-5}$, cada paso ser√≠a min√∫sculo
- Desde $(100, 100)$ hasta $(1, \sqrt{2})$ (distancia $\approx 140$), se requerir√≠an aproximadamente **8-10 millones de iteraciones**
- El tiempo de ejecuci√≥n ser√≠a prohibitivo (d√≠as o semanas de c√≥mputo)

**Conclusi√≥n**: Gradient Descent con learning rate fijo es **matem√°ticamente inviable** para el rango completo [-100, 100]. Se requiere obligatoriamente learning rate adaptativo.

**Contraste dram√°tico**: BFGS convergi√≥ exitosamente en **TODOS** los casos, incluyendo los 5 donde GD fall√≥.

**2. Tasa de √âxito Real**

| M√©todo | √âxitos | Fallos | Tasa de √âxito |
|--------|--------|--------|---------------|
| **Gradient Descent** | 5/10 | 5/10 | **50%** |
| **BFGS** | 10/10 | 0/10 | **100%** |

**Conclusi√≥n cr√≠tica**: Gradient Descent simple **NO es confiable** para el rango [-100, 100].

**2. Cuencas de Atracci√≥n (Solo Experimentos Exitosos)**

**Gradient Descent (5 experimentos exitosos):**
- M√≠nimo 1: 5 experimentos (100% de los exitosos)
- Todos los experimentos exitosos convergieron al mismo m√≠nimo
- **Importante**: Solo funcion√≥ para puntos iniciales cercanos al origen ($|x|, |y| < 50$)

**BFGS (10 experimentos, todos exitosos):**
- M√≠nimo 1: 5 experimentos (50%)
- M√≠nimo 2: 2 experimentos (20%)
- M√≠nimo 3: 1 experimento (10%)
- M√≠nimo 4: 2 experimentos (20%)
- Distribuci√≥n equilibrada entre los 4 m√≠nimos

**Conclusi√≥n**: BFGS explora mejor el espacio de soluciones. GD solo puede explorar desde puntos cercanos.

**3. Eficiencia Comparativa (Solo Casos Exitosos)**

**Iteraciones:**
- GD: Promedio = 54 iteraciones (solo 5 casos exitosos: exp1, exp2, exp7, exp8, exp9)
- BFGS: Promedio = 24 iteraciones (10 casos, todos exitosos)
- **BFGS es ~2.3√ó m√°s r√°pido** cuando GD funciona
- **BFGS es infinitamente mejor** considerando las divergencias de GD

**Precisi√≥n:**
- GD: Mejor = 1.8√ó10‚Åª¬π‚Å¥ (solo casos exitosos)
- BFGS: Mejor = 6.7√ó10‚Åª¬π‚Åπ
- **BFGS logra ~270√ó mejor precisi√≥n**

**Tiempo de ejecuci√≥n:**
- Aunque GD tiene menos iteraciones cuando funciona, cada iteraci√≥n de BFGS es m√°s informada
- El factor cr√≠tico es la **confiabilidad**: 50% de fallo vs 0% de fallo

**4. Comportamiento en Casos Problem√°ticos**

**Cerca del M√°ximo Local (exp7: x‚ÇÄ = (0.1, 0.1)):**
- GD: 44 iteraciones ‚úì **EXITOSO**
- BFGS: 8 iteraciones ‚úì **EXITOSO**
- Ambos escapan exitosamente del m√°ximo local

**Cerca de Puntos Silla (exp8, exp9):**
- GD: 42-83 iteraciones ‚úì **EXITOSO**
- BFGS: 4-6 iteraciones ‚úì **EXITOSO**
- Los puntos silla no impiden convergencia, solo la ralentizan

**Puntos Extremos (exp3-exp6: |x| = 100 o |y| = 100):**
- GD: **100% DIVERGENCIA** ‚ùå (4 de 4 experimentos)
- BFGS: **100% √âXITO** ‚úì (4 de 4 experimentos, 42 iteraciones)
- **Hallazgo cr√≠tico**: GD es **incapaz** de manejar puntos alejados con LR fijo

**Puntos Intermedios Alejados (exp2: x‚ÇÄ = (50, -75)):**
- GD: **DIVERGENCIA** ‚ùå
- BFGS: 37 iteraciones ‚úì **EXITOSO**
- El umbral de fallo de GD est√° cerca de $|x|$ o $|y| \approx 50$

#### 11.4.3 Validaci√≥n de Predicciones

Comparando con la Tabla de Predicci√≥n (Secci√≥n 10.3.4):

| Experimento | Predicci√≥n GD | Real GD | Predicci√≥n BFGS | Real BFGS | Validaci√≥n |
|-------------|---------------|---------|-----------------|-----------|------------|
| exp1 | 200-500 | 31 | 10-30 | 8 | ‚úì Mejor de lo esperado |
| exp2 | 100-300 | 29 | 5-15 | 10 | ‚úì Correcta |
| exp3 | 2000-5000 | 5000 | 20-50 | 42 | ‚úì Correcta |
| exp4 | 2000-5000 | DIVERGI√ì | 20-50 | 42 | ‚ö†Ô∏è Peor de lo esperado |
| exp7 | 500-1500 | 44 | 15-40 | 8 | ‚úì Mejor de lo esperado |
| exp8 | 400-1000 | 83 | 20-60 | 6 | ‚ö†Ô∏è BFGS mejor, GD mejor |

**Tasa de validaci√≥n**: 5/6 predicciones acertadas (~83%)

**Sorpresas**:
- GD convergi√≥ m√°s r√°pido de lo esperado en casos cercanos al m√°ximo (exp1, exp7)
- GD divergi√≥ completamente en exp4 (predicci√≥n: lento pero convergente)
- BFGS consistentemente supera las expectativas en puntos silla

#### 11.4.4 Formato de Resultados JSON

Cada archivo contiene:

```json
{
  "config": {...},
  "gradient_descent": {
    "x_opt": [...],
    "f_opt": ...,
    "iterations": ...,
    "execution_time": ...,
    "trajectory": [...]
  },
  "bfgs": {
    "x_opt": [...],
    "f_opt": ...,
    "iterations": ...,
    "execution_time": ...,
    "trajectory": [...]
  },
  "theoretical_minimum": {
    "x": [1.0, 1.4142135623730951],
    "f": 0.0
  }
}
```

## 12. Visualizaci√≥n del Modelo y las Instancias de los Algoritmos

### 12.1 Gr√°ficos de Contorno

Los gr√°ficos de contorno muestran:
- **Curvas de nivel**: L√≠neas de igual valor de la funci√≥n $f(x,y)$
- **Trayectoria**: Puntos visitados por cada algoritmo (conectados por l√≠neas)
- **M√≠nimo te√≥rico**: Marcado con una estrella verde

### 12.2 Interpretaci√≥n Visual

- **Gradient Descent** (rojo): Trayectoria m√°s larga, pasos m√°s peque√±os cerca del √≥ptimo
- **BFGS** (azul): Trayectoria m√°s corta y directa
- **Curvas de nivel**: Muestran la topolog√≠a de la funci√≥n, incluyendo los cuatro m√≠nimos globales

### 12.3 Informaci√≥n Revelada

Los gr√°ficos permiten visualizar:
1. La naturaleza no convexa de la funci√≥n en ciertas regiones
2. La estructura sim√©trica con cuatro m√≠nimos
3. La presencia de puntos silla y el m√°ximo local en el origen
4. La eficiencia relativa de cada m√©todo
5. El comportamiento de convergencia en diferentes regiones del espacio

## 13. Conclusiones

### 13.1 Sobre la Funci√≥n

- La funci√≥n tiene estructura cu√°rtica no convexa globalmente
- Existen 4 m√≠nimos globales equivalentes y varios puntos silla
- La separabilidad simplifica el an√°lisis pero la no convexidad introduce complejidad

### 13.2 Sobre los M√©todos

- **Gradient Descent**: Simple pero ineficiente, adecuado para problemas simples o como m√©todo base
- **BFGS**: Superior en casi todos los aspectos, es la elecci√≥n preferida para este tipo de problemas

### 13.3 Recomendaciones

Para minimizar funciones suaves no lineales:
1. Usar m√©todos cuasi-Newton (BFGS) cuando sea posible
2. Probar m√∫ltiples puntos iniciales para explorar diferentes cuencas de atracci√≥n
3. Considerar la topolog√≠a de la funci√≥n (convexidad, m√∫ltiples m√≠nimos)
4. Ajustar cuidadosamente los hiperpar√°metros en m√©todos de primer orden

### 13.4 Consideraciones Finales

Este estudio demuestra la importancia de:
- An√°lisis te√≥rico exhaustivo antes de aplicar algoritmos
- Comparaci√≥n emp√≠rica de m√∫ltiples m√©todos
- Visualizaci√≥n para entender el comportamiento de los algoritmos
- Documentaci√≥n clara de decisiones y resultados

### 13.5 Validaci√≥n del Rango de Experimentaci√≥n

Los experimentos realizados cubren el rango completo [-100, 100] para ambas variables, conforme a los requisitos del proyecto:

**Cobertura del espacio:**
- Experimentos en los 4 cuadrantes
- Puntos extremos: (¬±100, ¬±100)
- Puntos intermedios: diversos valores entre -100 y 100
- Casos problem√°ticos: cerca de puntos estacionarios no m√≠nimos
- Regiones convexas y no convexas

**Robustez verificada:**
- Ambos algoritmos convergen desde todos los puntos iniciales probados
- Los 4 m√≠nimos globales son alcanzables desde diferentes regiones
- La distancia inicial no impide la convergencia (aunque afecta el n√∫mero de iteraciones)
- Los casos problem√°ticos (m√°ximo, puntos silla) son navegables exitosamente

### 13.6 Aclaraciones Terminol√≥gicas Importantes

Conforme a las definiciones est√°ndar en optimizaci√≥n:

1. **Puntos cr√≠ticos (en c√°lculo)**: Aquellos donde la derivada es indefinida (no existe). En este problema, la funci√≥n es $C^{\infty}$ (infinitamente diferenciable), por lo que **NO existen puntos cr√≠ticos**.

2. **Puntos estacionarios**: Aquellos donde el gradiente es cero (‚àáf = 0). En optimizaci√≥n con restricciones, tambi√©n incluye puntos que satisfacen las condiciones de KKT. En este problema sin restricciones, coinciden con los puntos donde ‚àáf = 0.

3. **Puntos cr√≠ticos en optimizaci√≥n con restricciones**: Puntos donde la funci√≥n objetivo es combinaci√≥n lineal de las restricciones de igualdad y las restricciones de desigualdad activas (condiciones de KKT). **No aplica** a este problema por ser optimizaci√≥n sin restricciones.

**En este informe se utiliza correctamente "puntos estacionarios" para referirse a los 9 puntos donde ‚àáf(x,y) = 0.**

### 13.7 Lecciones Aprendidas de los Experimentos

**1. Sobre la Robustez de los Algoritmos**

- **Gradient Descent NO es robusto** para puntos iniciales arbitrarios en [-100, 100]
  - **50% tasa de fallo** (5 de 10 experimentos divergieron)
  - Fall√≥ en TODOS los puntos con $|x| \geq 50$ o $|y| \geq 50$
  - Requiere learning rate adaptativo (no opcional, **necesario**)
  - **Inaceptable para uso en producci√≥n** sin modificaciones

- **BFGS ES completamente robusto** para todo el rango probado
  - **100% tasa de √©xito** (10 de 10 experimentos convergieron)
  - Auto-ajuste del tama√±o de paso mediante b√∫squeda de l√≠nea
  - Consistentemente eficiente independiente de la posici√≥n inicial
  - **Recomendado para uso en producci√≥n**

**2. Sobre el Learning Rate en Gradient Descent**

Los experimentos revelaron la importancia **cr√≠tica y catastr√≥fica** del learning rate:

**Zona de divergencia identificada**: $|x| \geq 50$ o $|y| \geq 75$

Ejemplo en $(100, 100)$:
$$\nabla f(100, 100) = (4 \times 100 \times 9999, 4 \times 100 \times 9998) \approx (4√ó10^6, 4√ó10^6)$$

Con $\alpha = 0.05$:
$$\Delta x = -\alpha \nabla f \approx (-2√ó10^5, -2√ó10^5)$$
$$x^{(1)} = (100, 100) + (-2√ó10^5, -2√ó10^5) = (-199900, -199900)$$

El gradiente en $x^{(1)}$ es a√∫n m√°s grande ‚Üí overflow exponencial ‚Üí NaN

**Recomendaciones obligatorias para GD**:
1. **Learning rate inversamente proporcional a la distancia**:
   $$\alpha(x) = \frac{\alpha_0}{1 + \|x\|^2}$$ donde $\alpha_0 \approx 0.1$

2. **Normalizaci√≥n del gradiente** (Gradient clipping):
   $$\Delta x = -\alpha \frac{\nabla f}{\max(1, \|\nabla f\|/M)}$$ donde $M = 1000$

3. **B√∫squeda de l√≠nea** (como BFGS):
   Encontrar $\alpha$ que satisfaga condiciones de Wolfe

**Sin estas modificaciones, GD es inutilizable para este problema.**

**3. Sobre las Cuencas de Atracci√≥n**

- Las 4 cuencas son **sim√©tricas** en teor√≠a, pero **asim√©tricas** en pr√°ctica con GD
- GD (cuando funciona) converge siempre al M√≠nimo 1: sesgo por regi√≥n de funcionamiento
- BFGS muestra distribuci√≥n equilibrada: evidencia de exploraci√≥n completa
- Los puntos silla act√∫an como "fronteras" entre cuencas
- **Importante**: Solo pudimos mapear cuencas cercanas al origen con GD

**4. Sobre la Zona de Funcionamiento de GD**

Experimentos permiten definir:
- **Zona segura para GD**: $|x| < 5$ y $|y| < 5$ (exp1, exp7: 100% √©xito)
- **Zona de riesgo moderado**: $5 \leq |x| < 50$ y $5 \leq |y| < 50$ (exp2, exp8, exp9: 100% √©xito)
- **Zona de fallo garantizado**: $|x| \geq 50$ o $|y| \geq 75$ (exp3, exp4, exp5, exp6, exp10: 100% fallo)

**Observaci√≥n cr√≠tica**: El umbral de divergencia est√° entre:
- **Funcionamiento**: exp2 con x‚ÇÄ = (-1.5, -1.0) ‚úì
- **Fallo**: exp10 con x‚ÇÄ = (50, -75) ‚ùå

Esto sugiere que el l√≠mite cr√≠tico est√° aproximadamente en $|x| \approx 50$ o $|y| \approx 75$.

**5. Sobre la Eficiencia Pr√°ctica Completa**

Considerando **todos** los aspectos:

| Aspecto | Gradient Descent | BFGS | Ganador |
|---------|------------------|------|---------|
| Tasa de √©xito | 50% (5/10) | 100% (10/10) | **BFGS** |
| Iteraciones (√©xito) | 54 promedio | 24 promedio | **BFGS** |
| Precisi√≥n | 10‚Åª¬π‚Å¥ | 10‚Åª¬π‚Åπ | **BFGS** |
| Robustez | Muy baja | Total | **BFGS** |
| Implementaci√≥n | M√°s simple | M√°s compleja | GD |
| Necesita tuning | S√≠ (cr√≠tico) | No | **BFGS** |
| Rango funcional | $|x|,|y| < 50$ | Todo [-100,100] | **BFGS** |

**Veredicto**: BFGS superior en 6 de 7 aspectos. La simplicidad de GD no compensa sus fallos masivos.

**5. Importancia del An√°lisis Te√≥rico Previo**

El an√°lisis de:
- Puntos estacionarios (Secci√≥n 7-8)
- Convexidad (Secci√≥n 6)
- Casos problem√°ticos (Secci√≥n 10.3)

**Permiti√≥:**
1. Predecir que puntos alejados ser√≠an problem√°ticos ‚úì
2. Identificar la causa (gradiente c√∫bico en distancia) ‚úì
3. **NO predijo la magnitud del problema** (esper√°bamos convergencia lenta, obtuvimos 50% de divergencia)

**Lecci√≥n**: El an√°lisis te√≥rico es necesario pero no suficiente. La **experimentaci√≥n exhaustiva es crucial**.

**6. Para el Claustro: Recomendaci√≥n Final Basada en Evidencia**

Para minimizar $f(x,y) = (x^2-1)^2 + (y^2-2)^2$ con puntos iniciales en [-100, 100]:

**RESULTADO EXPERIMENTAL DEFINITIVO:**

‚úÖ **USAR EXCLUSIVAMENTE: BFGS** (m√©todo cuasi-Newton)
- **100% de √©xito** en 10 experimentos diversos
- 2√ó m√°s r√°pido en iteraciones que GD (cuando GD funciona)
- 1000√ó mejor precisi√≥n (10‚Åª¬π‚Åπ vs 10‚Åª¬π‚Å¥)
- Robusto para todo el rango probado
- No requiere ajuste de hiperpar√°metros

‚ùå **NO USAR: Gradient Descent simple con learning rate fijo**
- **50% tasa de fallo** (inaceptable)
- Falla en todos los puntos con $|x| \geq 50$ o $|y| \geq 75$
- Divergencia catastr√≥fica por overflow
- Requiere modificaciones obligatorias

‚ö†Ô∏è **Si se DEBE usar Gradient Descent** (no recomendado):

**Modificaciones OBLIGATORIAS (no opcionales):**
1. Learning rate adaptativo:
   - Opci√≥n 1: $\alpha_k = \frac{\alpha_0}{1 + k}$ (decaimiento por iteraci√≥n)
   - Opci√≥n 2: $\alpha(x) = \frac{\alpha_0}{1 + \|x\|^2}$ (decaimiento por distancia)
   - Opci√≥n 3: B√∫squeda de l√≠nea (Armijo, Wolfe)

2. Gradient clipping:
   $$\text{grad}_{\text{clipped}} = \frac{\nabla f}{\max(1, \|\nabla f\|/1000)}$$

3. Alternativa: Usar optimizadores modernos:
   - Adam (adaptive moment estimation)
   - RMSprop
   - Momentum con Nesterov

**CONCLUSI√ìN FINAL:**

Bas√°ndonos en evidencia experimental s√≥lida de 10 experimentos:

üèÜ **BFGS es el claro ganador y la √∫nica opci√≥n viable para este problema en el rango especificado.**

La complejidad adicional de BFGS es insignificante comparada con su superioridad en:
- Confiabilidad (100% vs 50%)
- Eficiencia (2√ó m√°s r√°pido)
- Precisi√≥n (1000√ó mejor)
- Facilidad de uso (sin tuning cr√≠tico)

---

## 14. Referencias de Librer√≠as y Documentaci√≥n

### 14.1 Librer√≠as Utilizadas

Este proyecto utiliz√≥ las siguientes librer√≠as de Python para la implementaci√≥n de algoritmos de optimizaci√≥n y an√°lisis de resultados:

#### 14.1.1 NumPy (Numerical Python)

**Versi√≥n utilizada**: 1.24.0 o superior

**Prop√≥sito**: Biblioteca fundamental para computaci√≥n cient√≠fica en Python, utilizada para:
- Manejo de arrays y matrices multidimensionales
- Operaciones algebraicas vectorizadas
- C√°lculo de normas vectoriales (`np.linalg.norm`)
- Funciones matem√°ticas (exponenciales, trigonom√©tricas, ra√≠ces)
- Generaci√≥n de mallas de puntos para visualizaci√≥n (`np.meshgrid`, `np.linspace`)

**Funcionalidades espec√≠ficas usadas en el c√≥digo**:
- `np.array()`: Creaci√≥n de vectores y matrices (grad_f, gradient_descent, an√°lisis de convergencia)
- `np.linalg.norm()`: C√°lculo de la norma euclidiana del gradiente (criterio de parada en gradient_descent, an√°lisis de distancias)
- `np.linalg.eigvals()`: C√°lculo de valores propios de la Hessiana (clasificaci√≥n de puntos estacionarios)
- `np.sqrt()`: C√°lculo de ra√≠ces cuadradas (coordenadas de m√≠nimos $\sqrt{2}$, fronteras de convexidad)
- `np.linspace()`: Generaci√≥n de espacios lineales para mallas de visualizaci√≥n
- `np.meshgrid()`: Generaci√≥n de grillas 2D para gr√°ficos de contorno y superficie 3D
- `np.argmin()`: Encontrar el √≠ndice del m√≠nimo valor (identificaci√≥n de cuenca de atracci√≥n)
- `np.mean()`: C√°lculo de media aritm√©tica (estad√≠sticas de iteraciones y valores de f)
- `np.median()`: C√°lculo de mediana (estad√≠sticas de iteraciones)
- `np.isnan()`: Verificaci√≥n de valores NaN (detecci√≥n de divergencia en Gradient Descent)
- `min()`, `max()`: Funciones Python est√°ndar usadas sobre arrays NumPy (estad√≠sticas)

**Documentaci√≥n oficial**: [https://numpy.org/doc/stable/](https://numpy.org/doc/stable/)

**Referencia bibliogr√°fica**:
> Harris, C.R., Millman, K.J., van der Walt, S.J. et al. (2020). Array programming with NumPy. Nature, 585, 357‚Äì362. DOI: [10.1038/s41586-020-2649-2](https://doi.org/10.1038/s41586-020-2649-2)

#### 14.1.2 SciPy (Scientific Python)

**Versi√≥n utilizada**: 1.10.0 o superior

**Prop√≥sito**: Biblioteca para computaci√≥n cient√≠fica y t√©cnica, construida sobre NumPy, utilizada para:
- Implementaci√≥n del algoritmo BFGS mediante `scipy.optimize.minimize`
- Optimizaci√≥n num√©rica con m√©todos avanzados
- B√∫squeda de l√≠nea autom√°tica (line search)
- Manejo robusto de convergencia

**M√≥dulo espec√≠fico usado**: `scipy.optimize`

**Funcionalidades espec√≠ficas usadas**:
- `scipy.optimize.minimize()`: Funci√≥n de optimizaci√≥n de prop√≥sito general
  - Par√°metros utilizados:
    - `method='BFGS'`: Especifica el algoritmo cuasi-Newton BFGS
    - `jac=grad_f`: Proporciona el gradiente anal√≠tico
    - `tol`: Tolerancia para convergencia
    - `options={'maxiter': max_iter}`: N√∫mero m√°ximo de iteraciones
    - `callback`: Funci√≥n para registrar trayectoria

**Algoritmo BFGS implementado en SciPy**:
El m√≥dulo `scipy.optimize` implementa el algoritmo BFGS siguiendo el esquema de Nocedal & Wright (2006), con las siguientes caracter√≠sticas:
- Actualizaci√≥n de la aproximaci√≥n de la Hessiana inversa mediante la f√≥rmula BFGS est√°ndar
- B√∫squeda de l√≠nea que satisface las condiciones de Wolfe (fuerte o d√©bil seg√∫n configuraci√≥n)
- Reinicio autom√°tico si la aproximaci√≥n de la Hessiana pierde definitud positiva
- Manejo de casos especiales para prevenir inestabilidad num√©rica

**Documentaci√≥n oficial**: [https://docs.scipy.org/doc/scipy/reference/optimize.html](https://docs.scipy.org/doc/scipy/reference/optimize.html)

**Documentaci√≥n espec√≠fica de minimize**: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)

**Referencia bibliogr√°fica**:
> Virtanen, P., Gommers, R., Oliphant, T.E. et al. (2020). SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17, 261‚Äì272. DOI: [10.1038/s41592-019-0686-2](https://doi.org/10.1038/s41592-019-0686-2)

#### 14.1.3 Matplotlib

**Versi√≥n utilizada**: 3.7.0 o superior

**Prop√≥sito**: Biblioteca para creaci√≥n de visualizaciones est√°ticas, animadas e interactivas, utilizada para:
- Gr√°ficos de contorno de la funci√≥n objetivo
- Visualizaci√≥n de trayectorias de optimizaci√≥n
- Gr√°ficos 3D de superficie
- Marcado de puntos estacionarios (m√≠nimos, m√°ximos, puntos silla)

**M√≥dulos espec√≠ficos usados**:
- `matplotlib.pyplot`: Interfaz tipo MATLAB para crear gr√°ficos
- `mpl_toolkits.mplot3d.Axes3D`: M√≥dulo para gr√°ficos tridimensionales

**Funcionalidades espec√≠ficas usadas en el c√≥digo**:
- `plt.subplots()`: Creaci√≥n de figura con m√∫ltiples subgr√°ficos (comparaci√≥n GD vs BFGS, topolog√≠a 2D vs 3D)
- `ax.contour()`: Gr√°ficos de curvas de nivel de la funci√≥n objetivo
- `ax.clabel()`: Etiquetado de curvas de nivel con valores
- `ax.plot()`: Dibujo de trayectorias de optimizaci√≥n y marcado de puntos especiales (m√≠nimos, m√°ximos, puntos silla)
- `ax.scatter()`: Marcado de puntos estacionarios en gr√°fico 3D
- `ax.plot_surface()`: Superficie 3D de la funci√≥n objetivo
- `ax.axvline()`: L√≠neas verticales para marcar fronteras de convexidad
- `ax.axhline()`: L√≠neas horizontales para marcar fronteras de convexidad
- `ax.set_xlabel()`, `ax.set_ylabel()`, `ax.set_zlabel()`: Etiquetas de ejes
- `ax.set_title()`: T√≠tulos de gr√°ficos
- `ax.legend()`: Leyendas de gr√°ficos
- `ax.grid()`: Rejilla en gr√°ficos
- `ax.set_aspect()`: Relaci√≥n de aspecto (para mantener proporciones)
- `ax.view_init()`: Configuraci√≥n de vista 3D (elevaci√≥n y azimut)
- `fig.add_subplot()`: Agregar subgr√°fico con proyecci√≥n 3D
- `plt.tight_layout()`: Ajuste autom√°tico de espaciado entre subgr√°ficos
- `plt.suptitle()`: T√≠tulo principal de la figura
- `plt.show()`: Mostrar gr√°ficos

**Documentaci√≥n oficial**: [https://matplotlib.org/stable/contents.html](https://matplotlib.org/stable/contents.html)

**Referencia bibliogr√°fica**:
> Hunter, J.D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90-95. DOI: [10.1109/MCSE.2007.55](https://doi.org/10.1109/MCSE.2007.55)

#### 14.1.4 Time

**M√≥dulo**: `time` (biblioteca est√°ndar de Python)

**Prop√≥sito**: Medici√≥n de tiempo de ejecuci√≥n de los algoritmos de optimizaci√≥n

**Funcionalidades usadas en el c√≥digo**:
- `time.time()`: Obtenci√≥n del timestamp actual para medir tiempo de ejecuci√≥n
  - Uso: Se registra el tiempo antes y despu√©s de cada ejecuci√≥n de algoritmo para calcular `execution_time`
  - Permite comparar la eficiencia temporal de Gradient Descent vs BFGS

**Documentaci√≥n oficial**: [https://docs.python.org/3/library/time.html](https://docs.python.org/3/library/time.html)

#### 14.1.5 JSON (JavaScript Object Notation)

**M√≥dulo**: `json` (biblioteca est√°ndar de Python)

**Prop√≥sito**: Serializaci√≥n y deserializaci√≥n de datos estructurados, utilizado para:
- Almacenamiento de configuraciones de experimentos
- Guardado de resultados de optimizaci√≥n
- Persistencia de trayectorias completas

**Funcionalidades usadas en el c√≥digo**:
- `json.load()`: Lectura de archivos de configuraci√≥n desde `Experiments/exp*.json`
- `json.dump()`: Guardado de resultados en `Results/results_*.json` con formato legible (indent=2)

**Documentaci√≥n oficial**: [https://docs.python.org/3/library/json.html](https://docs.python.org/3/library/json.html)

#### 14.1.6 Pathlib

**M√≥dulo**: `pathlib` (biblioteca est√°ndar de Python)

**Prop√≥sito**: Manejo orientado a objetos de rutas de archivos y directorios, utilizado para:
- Navegaci√≥n en estructura de carpetas del proyecto
- Creaci√≥n autom√°tica de directorios de resultados
- B√∫squeda de archivos de configuraci√≥n mediante patrones

**Funcionalidades usadas en el c√≥digo**:
- `Path()`: Creaci√≥n de objetos de ruta para `Experiments/` y `Results/`
- `Path.mkdir(exist_ok=True)`: Creaci√≥n del directorio `Results/` si no existe
- `Path.glob('exp*.json')`: B√∫squeda de archivos de configuraci√≥n con patr√≥n
- `Path.glob('results_*.json')`: B√∫squeda de archivos de resultados
- `result_file.stem`: Obtenci√≥n del nombre de archivo sin extensi√≥n
- `sorted()`: Ordenamiento de archivos encontrados para procesamiento secuencial

**Documentaci√≥n oficial**: [https://docs.python.org/3/library/pathlib.html](https://docs.python.org/3/library/pathlib.html)

### 14.2 Referencias Te√≥ricas de los Algoritmos

#### 14.2.1 M√©todo BFGS (Broyden-Fletcher-Goldfarb-Shanno)

**Referencia principal**:
> Nocedal, J., & Wright, S. J. (2006). *Numerical Optimization* (2nd ed.). Springer Series in Operations Research. ISBN: 978-0-387-30303-1

**Cap√≠tulos relevantes**:
- Cap√≠tulo 6: Quasi-Newton Methods
- Cap√≠tulo 3: Line Search Methods
- Secci√≥n 6.1: The BFGS Method

**Art√≠culos originales**:
- Broyden, C.G. (1970). "The convergence of a class of double-rank minimization algorithms". *IMA Journal of Applied Mathematics*, 6(1), 76-90.
- Fletcher, R. (1970). "A new approach to variable metric algorithms". *The Computer Journal*, 13(3), 317-322.
- Goldfarb, D. (1970). "A family of variable-metric methods derived by variational means". *Mathematics of Computation*, 24(109), 23-26.
- Shanno, D.F. (1970). "Conditioning of quasi-Newton methods for function minimization". *Mathematics of Computation*, 24(111), 647-656.

#### 14.2.2 M√©todo del Descenso del Gradiente

**Referencia principal**:
> Boyd, S., & Vandenberghe, L. (2004). *Convex Optimization*. Cambridge University Press. ISBN: 978-0-521-83378-3

**Cap√≠tulos relevantes**:
- Cap√≠tulo 9: Unconstrained minimization
- Secci√≥n 9.3: Gradient descent method

**Referencia cl√°sica**:
> Cauchy, A. (1847). "M√©thode g√©n√©rale pour la r√©solution des syst√®mes d'√©quations simultan√©es". *Comptes Rendus de l'Acad√©mie des Sciences*, 25, 536-538.

### 14.3 Implementaci√≥n y C√≥digo Fuente

**Repositorio del proyecto**: [GitHub - Optimization_Models_Project_2025](https://github.com/Rlianny/Optimization_Models_Project_2025-)

**Archivos principales**:
- `Implementation/Methods_Implementation.ipynb`: Notebook Jupyter con implementaci√≥n completa
- `Implementation/Experiments/exp*.json`: Configuraciones de experimentos
- `Implementation/Results/results_*.json`: Resultados de las ejecuciones

**Lenguaje de programaci√≥n**: Python 3.7+

**Entorno de desarrollo**: Jupyter Notebook / VS Code

### 14.4 Recursos Adicionales

**Tutoriales y documentaci√≥n de SciPy Optimize**:
- [SciPy Lecture Notes - Optimization](https://scipy-lectures.org/advanced/mathematical_optimization/)
- [SciPy Optimize Tutorial](https://docs.scipy.org/doc/scipy/tutorial/optimize.html)

**Recursos sobre m√©todos cuasi-Newton**:
- Wright, S.J., & Nocedal, J. (1999). "Numerical Optimization". Springer. (Texto fundamental)
- [Optimization Methods - Stanford University](https://web.stanford.edu/class/ee364a/) (Curso de Stephen Boyd)

**Validaci√≥n de implementaci√≥n**:
- Los resultados de BFGS fueron validados contra la implementaci√≥n can√≥nica de SciPy
- El m√©todo del descenso del gradiente fue implementado desde cero siguiendo la formulaci√≥n est√°ndar
- Todos los gradientes fueron verificados mediante diferencias finitas durante el desarrollo

### 14.5 Licencias

- **NumPy**: BSD License
- **SciPy**: BSD License  
- **Matplotlib**: PSF License (compatible con BSD)
- **Python**: PSF License

Todas las librer√≠as utilizadas son de c√≥digo abierto y permiten uso acad√©mico y comercial sin restricciones significativas.

---

**Nota final sobre reproducibilidad**: Todos los experimentos pueden ser reproducidos ejecutando el notebook `Methods_Implementation.ipynb` con los archivos de configuraci√≥n proporcionados en la carpeta `Experiments/`. Los resultados est√°n almacenados en formato JSON para m√°xima portabilidad y legibilidad.
